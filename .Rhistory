plot(prior ~ theta_seq, type = "l", col = "red", ylim = c(0, ymax),
ylab = "Densidad o likelihood", xlab = expression(theta))
lines(like ~ theta_seq, col = "blue")
lines(post ~ theta_seq, col = "black")
text(0.9, ymax * 0.95, "Previa", col = "red")
text(0.9, ymax * 0.85, "Likelihood", col = "blue")
text(0.9, ymax * 0.75, "Posterior", col = "black")
ll2 <- function(alpha, beta) {
# calculamos theta en base a alpha, beta y z. Ahora theta no es un parámetro,
# sino una cantidad derivada
theta <- inv_logit(alpha + beta * d$z) # función logística, que restringe a
# theta en [0, 1]
ll <- sum(dbinom(d$y, size = 1, prob = theta, log = T))
return(ll)
}
# versión vectorizada, para darle vectores de alpha y beta
ll2vec <- function(alpha, beta) {
ll <- numeric(length(alpha))
for(i in 1:length(alpha)) {
ll[i] <- ll2(alpha[i], beta[i])
}
return(ll)
}
nside <- 250 # número de valores por parámetro.
# OJO: la grilla tendrá nside ^ 2 valores
# El rango de valores debería abarcar todo lo que sería razonable.
alpha_seq <- seq(-4, 4, length.out = nside)
beta_seq <- seq(-4, 4, length.out = nside)
partable <- expand.grid(alpha = alpha_seq,
beta = beta_seq)
head(partable)
partable$log_like <- ll2vec(partable$alpha, partable$beta)
# normalizamos, para luego poder comparar con la previa y la posterior.
area <- diff(alpha_seq[1:2]) * diff(beta_seq[1:2])
partable$like_norm <- normalize_dens(exp(partable$log_like), area = area)
ggplot(partable, aes(x = alpha, y = beta, fill = like_norm)) +
geom_tile() +
scale_fill_viridis() +
labs(fill = "Likelihood") +
xlab(expression(alpha)) +
ylab(expression(beta))
row_optim <- which.max(partable$log_like)
(coef_hat1 <- partable[row_optim, c("alpha", "beta")])
ll2_neg <- function(alpha, beta) -ll2(alpha, beta)
m2_mle <- mle2(ll2_neg,                   # función de -log_likelihood
start = list(alpha = 0, beta = 0), # valores iniciales
lower = list(alpha = -Inf, beta = -Inf),
upper = list(alpha = Inf, beta = Inf),
method = "L-BFGS-B")
# Ese método permite poner límites en más de 1D.
# En este caso es absurdo poner límites, pero sirve de ejemplo.
summary(m2_mle) # HAY ESTRELLITAS
coef(m2_mle)
bbmle::confint(m2_mle, level = 0.95)
# Metemos ambas estimaciones en un data.frame para ggplot
coef_hat_table <- data.frame(
alpha = c(coef_hat1[1, 1], coef(m2_mle)[1]),
beta = c(coef_hat1[1, 2], coef(m2_mle)[2]),
metodo = c("grilla", "mle2")
)
# ploteamos nuevamente, pero haciendo zoom sobre la zona no-nula de la
# likelihood e incluyendo las estimaciones puntuales
ggplot(partable, aes(x = alpha, y = beta, fill = like_norm)) +
geom_tile() +
scale_fill_viridis() +
labs(fill = "Likelihood") +
xlab(expression(alpha)) +
ylab(expression(beta)) +
# agregamos las estimaciones puntuales
geom_point(data = coef_hat_table, aes(alpha, beta, shape = metodo),
inherit.aes = F, size = 4) +
scale_shape_manual(values = c(1, 4)) +
xlim(-3, -1) + # esto hace zoom
ylim(1.5, 3.8)
# Parámetros de las previas
alpha_mu <- 0
alpha_sigma <- 0.25
beta_mu <- 2
beta_sigma <- 0.25
# Previa
partable$log_prior <-
dnorm(partable$alpha, mean = alpha_mu, sd = alpha_sigma, log = T) +
dnorm(partable$beta, mean = beta_mu, sd = beta_sigma, log = T)
partable$prior <- exp(partable$log_prior)
# Posterior
partable$log_post <- partable$log_like + partable$log_prior
partable$post <- normalize_dens(exp(partable$log_post), area)
# Ordenamos para plotear
nn <- which(names(partable) %in% c("like_norm", "prior", "post"))
ptlong <- pivot_longer(partable,
cols = all_of(nn), names_to = "var",
values_to = "densidad")
ptlong$var <- factor(ptlong$var, levels = c("like_norm", "prior", "post"),
labels = c("Likelihood", "Previa", "Posterior"))
ggplot(ptlong, aes(x = alpha, y = beta, fill = densidad)) +
geom_tile(na.rm = T) +
scale_fill_viridis(na.value = "transparent") +
labs(fill = "Densidad") +
facet_wrap(vars(var), nrow = 2, axes = "all", axis.labels = "margins") +
xlab(expression(alpha)) +
ylab(expression(beta)) +
theme(strip.background = element_rect(color = "white"),
legend.position.inside = c(0.75, 0.25)) +
xlim(-3, 1.75) +
ylim(1, 3.8)
# Parámetros de las previas
alpha_mu <- 0
alpha_sigma <- 0.25
beta_mu <- 2
beta_sigma <- 1
# Previa
partable$log_prior <-
dnorm(partable$alpha, mean = alpha_mu, sd = alpha_sigma, log = T) +
dnorm(partable$beta, mean = beta_mu, sd = beta_sigma, log = T)
partable$prior <- exp(partable$log_prior)
# Posterior
partable$log_post <- partable$log_like + partable$log_prior
partable$post <- normalize_dens(exp(partable$log_post), area)
# Ordenamos para plotear
nn <- which(names(partable) %in% c("like_norm", "prior", "post"))
ptlong <- pivot_longer(partable,
cols = all_of(nn), names_to = "var",
values_to = "densidad")
ptlong$var <- factor(ptlong$var, levels = c("like_norm", "prior", "post"),
labels = c("Likelihood", "Previa", "Posterior"))
ggplot(ptlong, aes(x = alpha, y = beta, fill = densidad)) +
geom_tile(na.rm = T) +
scale_fill_viridis(na.value = "transparent") +
labs(fill = "Densidad") +
facet_wrap(vars(var), nrow = 2, axes = "all", axis.labels = "margins") +
xlab(expression(alpha)) +
ylab(expression(beta)) +
theme(strip.background = element_rect(color = "white"),
legend.position.inside = c(0.75, 0.25)) +
xlim(-3, 1.75) +
ylim(1, 3.8)
# Volvemos a definir previas, así nos independizamos de las elecciones que
# hicimos para el gráfico. También hay que recalcular la posterior
alpha_mu <- 0
alpha_sigma <- 5
beta_mu <- 0
beta_sigma <- 2
# Previa
partable$log_prior <-
dnorm(partable$alpha, mean = alpha_mu, sd = alpha_sigma, log = T) +
dnorm(partable$beta, mean = beta_mu, sd = beta_sigma, log = T)
partable$prior <- exp(partable$log_prior)
# Posterior
partable$log_post <- partable$log_like + partable$log_prior
partable$post <- normalize_dens(exp(partable$log_post), area)
# Igual que antes, sorteamos filas de partable según la probabilidad posterior.
npost <- 10000
ids_post <- sample(1:nrow(partable), size = npost, replace = T,
prob = partable$post)
m2_draws_hand <- as.matrix(partable[ids_post, c("alpha", "beta")])
# compilamos el modelo
smodel2 <- stan_model("modelo2.stan", verbose = F)
# preparamos datos
sdata2 <- list(
n = nrow(d), y = d$y, z = d$z,
alpha_mu = alpha_mu,
alpha_sigma = alpha_sigma,
beta_mu = beta_mu,
beta_sigma = beta_sigma
)
# muestreamos
m2 <- sampling(
smodel2, data = sdata2, seed = 9663,
cores = ncores, chains = 4, iter = 2000, warmup = 1000
)
# Chequeamos convergencia y calidad de las muestras
sm2 <- summary(m2)[[1]]
sm2[c("alpha", "beta"), c("n_eff", "Rhat")]
m2_draws <- as.matrix(m2, pars = c("alpha", "beta"))
# Comparamos muestreo a mano con el de Stan
# Distribuciones marginales
par(mfrow = c(1, 2))
plot(density(m2_draws[, "alpha"]), main = NA, xlab = expression(alpha))
lines(density(m2_draws_hand[, "alpha"]), col = "blue")
plot(density(m2_draws[, "beta"]), main = NA, xlab = expression(beta))
lines(density(m2_draws_hand[, "beta"]), col = "blue")
par(mfrow = c(1, 1))
# Distribución conjunta
xr <- range(rbind(m2_draws, m2_draws_hand)[, "beta"])
xr[1] <- xr[1] - abs(diff(xr)) * 0.05
xr[2] <- xr[2] + abs(diff(xr)) * 0.05
yr <- range(rbind(m2_draws, m2_draws_hand)[, "alpha"])
yr[1] <- yr[1] - abs(diff(yr)) * 0.05
yr[2] <- yr[2] + abs(diff(yr)) * 0.05
par(mfrow = c(1, 2))
plot(m2_draws[, "alpha"] ~ m2_draws[, "beta"], main = "Stan",
ylab = expression(alpha), xlab = expression(beta),
col = rgb(0, 0, 0, 0.1), pch = 19, ylim = yr, xlim = xr)
plot(m2_draws_hand[, "alpha"] ~ m2_draws_hand[, "beta"], main = "A mano",
ylab = expression(alpha), xlab = expression(beta),
col = rgb(0, 0, 0, 0.1), pch = 19, ylim = yr, xlim = xr)
par(mfrow = c(1, 1))
dfdraws <- rbind(
m2_draws_hand |> as.data.frame(),
m2_draws[, c("alpha", "beta")] |> as.data.frame()
)
dfdraws$metodo <- rep(c("A mano", "Stan"), c(npost, nrow(m2_draws)))
ggplot(dfdraws, aes(alpha, beta)) +
geom_hdr(method = method_kde(adjust = 1.5),
probs = c(0.99, seq(0.95, 0.05, by = -0.2), 0.05)) +
facet_wrap(vars(metodo), axes = "all", axis.labels = "margins") +
nice_theme() +
ylab(expression(beta)) +
xlab(expression(alpha))
nsim <- 1000 # nro de sets de datos a simular
# Modelo estimado por maximum likelihood.
coef_mle <- coef(m2_mle)
ysim_mle <- matrix(NA, nrow(d), nsim)
theta_fitted <- inv_logit(coef_mle["alpha"] + coef_mle["beta"] * d$z)
for(i in 1:nsim) {
ysim_mle[, i] <- rbinom(n = nrow(d), size = 1, prob = theta_fitted)
}
# Modelo estimado con enfoque Bayesiano, con Stan
ysim_stan <- matrix(NA, nrow(d), nsim)
# para considerar incertidumbre, cada vez que simulamos datos tomaremos una
# muestra de la posterior
npost_stan <- nrow(m2_draws)
# sorteamos 1000 muestras previamente
ids_res <- sample(1:npost_stan, size = nsim, replace = F)
for(i in 1:nsim) {
row <- ids_res[i]
theta_fitted_i <- inv_logit(
m2_draws[row, "alpha"] +
m2_draws[row, "beta"] * d$z
)
ysim_stan[, i] <- rbinom(n = nrow(d), size = 1, prob = theta_fitted_i)
}
# Calculamos residuos
res2_mle <- createDHARMa(observedResponse = d$y,
simulatedResponse = ysim_mle)
res2_stan <- createDHARMa(observedResponse = d$y,
simulatedResponse = ysim_stan)
plot(res2_mle)
plot(res2_stan)
# Residuos en función de la predictora (z)
plotResiduals(res2_mle, form = d$z, rank = F)
plotResiduals(res2_stan, form = d$z, rank = F)
nseq <- 200
zseq <- seq(0, 1, length.out = nseq)
# La predicción basada en la estimación puntual es la siguiente:
theta_pred_mle <- inv_logit(coef_mle["alpha"] + coef_mle["beta"] * zseq)
theta_logit <- coef_mle["alpha"] + coef_mle["beta"] * zseq
V <- vcov(m2_mle)
# matriz de varianza covarianza de la estimación
# (Normal Multivariada centrada en el MLE)
X <- cbind(rep(1, nseq), zseq)
# matriz de diseño para la predicción
theta_logit_se <- sqrt(diag(X %*% V %*% t(X)))
# standard error de la predicción.
# Esto es lo que hace la función predict(..., se.fit = T)
# cuando le pasamos un GLM.
# Ponemos todo en un data.frame
pred_mle <- data.frame(
z = zseq,
theta = theta_pred_mle,
theta_lower = inv_logit(theta_logit - qnorm(0.975) * theta_logit_se),
theta_upper = inv_logit(theta_logit + qnorm(0.975) * theta_logit_se)
)
ggplot(pred_mle, aes(z, theta, ymin = theta_lower, ymax = theta_upper)) +
geom_ribbon(color = NA, alpha = 0.3) +
geom_line() +
ylim(0, 1) +
ylab(expression(theta))
# Como tenemos 4000 muestras, generaremos una matriz de predicciones con 4000
# columnas
theta_pred <- matrix(NA, nseq, npost_stan)
# acá hacemos la misma cuenta para theta que hicimos arriba para simular datos,
# pero usando zseq en vez de d$z
for(i in 1:npost_stan) {
theta_pred[, i] <- inv_logit(m2_draws[i, "alpha"] +
m2_draws[i, "beta"] * zseq)
}
plot(density(theta_pred[1, ]))
plot(density(theta_pred[50, ]))
plot(theta_pred[, 1] ~ zseq, ylim = c(0, 1), ylab = expression(theta),
xlab = "z", col = rgb(0, 0, 0, 0.2), type = "l")
for(i in sample(1:npost_stan, 20)) {
lines(theta_pred[, i] ~ zseq, col = rgb(0, 0, 0, 0.3))
}
pred_stan <- as.data.frame(cbind(
z = zseq,
apply(theta_pred, 1, mean_ci) |> t() # esto resume con media e ic
))
ggplot(pred_stan, aes(z, mean, ymin = lower, ymax = upper)) +
geom_ribbon(color = NA, alpha = 0.3) +
geom_line() +
ylim(0, 1) +
ylab(expression(theta))
colnames(pred_stan) <- colnames(pred_mle)
pred_stan$approach <- "Bayesiano"
pred_mle$approach <- "Frecuentista"
pred_bayentista <- rbind(pred_stan, pred_mle)
ggplot(pred_bayentista,
aes(z, theta, ymin = theta_lower, ymax = theta_upper)) +
geom_ribbon(color = NA, alpha = 0.3) +
geom_line() +
facet_wrap(vars(approach), nrow = 1, axes = "all", axis.labels = "margins") +
ylim(0, 1) +
ylab(expression(theta)) +
nice_theme()
## Ordenamos los datos en forma matricial
nt <- max(d$t)
nf <- max(d$grupo) # f de follow, seguimiento.
y <- matrix(d$y, nt, nf)
x <- matrix(d$x, nt, nf)
# z es latente! no la vimos.
# Función de likelihood
ll3 <- function(params) {
# Extraemos los parámetros del vector params
alpha <- params[1]
beta <- params[2]
iota <- params[3]
delta <- params[4]
z1 <- params[5:(nf+4)]
# matriz de predictora latente
z <- matrix(NA, nt, nf)
z[1, ] <- z1           # primer estado se estima
theta <- matrix(NA, nt, nf) # cantidad derivada
theta[1, ] <- inv_logit(alpha + beta * z[1, ])
# Loop recursivo para calcular theta y z
for(t in 2:nt) {
# actualizamos z en base al ataque en el intervalo anterior
z[t, ] <-
z[t-1, ] +
iota * (1 - z[t-1, ]) * x[t-1, ] -  # ataque (aumenta)
delta * z[t-1, ] * (1 - x[t-1, ])   # no ataque (decrece)
theta[t, ] <- inv_logit(alpha + beta * z[t, ])
}
# vectorizamos theta, para calcular likelihood.
ll <- sum(dbinom(d$y, size = 1, prob = as.vector(theta), log = T))
return(ll)
}
# La negamos para mle2
ll3_neg <- function(x) -ll3(x)
# nombres
parnames1 <- c("alpha", "beta", "iota", "delta")
parnames2 <- paste("z1", 1:nf, sep = "_")
parnames <- c(parnames1, parnames2)
# valores iniciales
start1 <- c(0, 1, 0.5, 0.5)
start2 <- rep(0.5, nf) # para z1
start <- c(start1, start2)
names(start) <- parnames
# bounds
lower1 <- c(-Inf, -Inf, 0, 0)
upper1 <- c(Inf, Inf, 1, 1)
lower2 <- rep(0, nf)
upper2 <- rep(1, nf)
lower <- c(lower1, lower2)
upper <- c(upper1, upper2)
names(lower) <- names(upper) <- parnames
# Estimamos
parnames(ll3_neg) <- parnames
m3_mle <- mle2(ll3_neg, start = start, parnames = parnames, vecpar = T,
lower = lower, upper = upper, method = "L-BFGS-B")
summary(m3_mle) # Hay problemas. Mejor ser bayesianos (frecuentistas, ver
# la sección de likelihood penalizada).
smodel3 <- stan_model("modelo3.stan", verbose = F)
# preparamos datos
sdata3 <- list(
nt = nt, nf = nf,
y = d$y, # vector
x = x,   # matriz
# previas
alpha_mu = alpha_mu,
alpha_sigma = alpha_sigma,
beta_mu = beta_mu,
beta_sigma = beta_sigma
)
# muestreamos
m3 <- sampling(
smodel3, data = sdata3, seed = 9663,
cores = ncores, chains = 4, iter = 2000, warmup = 1000,
pars = c("alpha", "beta", "iota", "delta", "z1",
"z_vec", "theta_vec")
)
# Chequeamos convergencia y calidad de las muestras
sm3 <- summary(m3)[[1]]
min(sm3[, "n_eff"])
max(sm3[, "Rhat"])
nsim <- 1000 # nro de sets de datos a simular
ysim_stan <- matrix(NA, nrow(d), nsim)
npost_stan <- nrow(m3_draws)
m3_draws <- as.matrix(m3)
plot(density(m3_draws[, "alpha"]), main = NA, xlab = expression(alpha))
curve(dnorm(x, alpha_mu, alpha_sigma), add = T, col = 2)
plot(density(m3_draws[, "beta"]), main = NA, xlab = expression(beta))
curve(dnorm(x, beta_mu, beta_sigma), add = T, col = 2)
plot(density(m3_draws[, "iota"], from = 0, to = 1),
main = NA, xlab = expression(iota))
curve(dunif(x, 0, 1), add = T, col = 2)
plot(density(m3_draws[, "delta"], from = 0, to = 1),
main = NA, xlab = expression(delta))
curve(dunif(x, 0, 1), add = T, col = 2)
z1_draws <- m3_draws[, grep("z1", colnames(m3_draws))]
colorcines <- viridis(nf, option = "H")
plot(density(z1_draws[, 1], from = 0, to = 1, adjust = 1.5), main = NA, xlab = "z1",
ylim = c(0, 4.5), col = colorcines[1])
for(i in 2:nf) {
lines(density(z1_draws[, i], from = 0, to = 1, adjust = 1.5),
col = colorcines[i])
}
curve(dunif(x, 0, 1), add = T, col = "black", lwd = 1.5, lty = 2)
zvec_draws <- m3_draws[, grep("z_vec", colnames(m3_draws))]
cols_nt <- d$t == nt
znt <- zvec_draws[, cols_nt] # sólo los z para t = nt
# Graficamos comparando z1 con z12.
f <- sample(1:nf, size = 1) # muestreamos seguimiento
dd1 <- density(z1_draws[, f], from = 0, to = 1)
dd12 <- density(znt[, f], from = 0, to = 1)
yy <- max(c(dd1$y, dd12$y))
plot(dd1, ylim = c(0, yy), main = f, xlab = "z")
lines(dd12, col = "red")
nsim <- 1000 # nro de sets de datos a simular
ysim_stan <- matrix(NA, nrow(d), nsim)
npost_stan <- nrow(m3_draws)
z_draws <- m3_draws[, grep("z_vec", colnames(m3_draws))]
ids_res <- sample(1:npost_stan, size = nsim, replace = F)
for(i in 1:nsim) {
row <- ids_res[i]
theta_fitted_i <- inv_logit(
m3_draws[row, "alpha"] +
m3_draws[row, "beta"] * z_draws[row, ]
)
ysim_stan[, i] <- rbinom(n = nrow(d), size = 1, prob = theta_fitted_i)
}
res3_stan <- createDHARMa(observedResponse = d$y,
simulatedResponse = ysim_stan)
plot(res3_stan)
# Residuos en función de la predictora (z)
zhat <- colMeans(z_draws)
plotResiduals(res3_stan, form = zhat, rank = F)
nseq <- 200
zseq <- seq(0, 1, length.out = nseq)
theta_pred <- matrix(NA, nseq, npost_stan)
for(i in 1:npost_stan) {
theta_pred[, i] <- inv_logit(m3_draws[i, "alpha"] +
m3_draws[i, "beta"] * zseq)
}
pred_stan <- as.data.frame(cbind(
z = zseq,
apply(theta_pred, 1, mean_ci) |> t() # esto resume con media e ic
))
ggplot(pred_stan, aes(z, mean, ymin = lower, ymax = upper)) +
geom_ribbon(color = NA, alpha = 0.3) +
geom_line() +
ylim(0, 1) +
ylab(expression(theta))
ntpred <- 18 # pasos para predecir
zinc <- matrix(0, ntpred, npost_stan) # aumento
zdec <- matrix(1, ntpred, npost_stan) # decaimiento
tinc <- zinc # t por theta, prob de movimiento.
tdec <- zinc
iota_hat <- m3_draws[, "iota"]
delta_hat <- m3_draws[, "delta"]
alpha_hat <- m3_draws[, "alpha"]
beta_hat <- m3_draws[, "beta"]
# Iniciamos los theta
tinc[1, ] <- inv_logit(alpha_hat + beta_hat * zinc[1, ]) # z = 1
tdec[1, ] <- inv_logit(alpha_hat + beta_hat * zdec[1, ]) # z = 0
for(t in 2:ntpred) {
# aumento bajo ataque constante
zinc[t, ] <- zinc[t-1, ] + iota_hat * (1 - zinc[t-1, ])
tinc[t, ] <- inv_logit(alpha_hat + beta_hat * zinc[t, ])
# decaimiento bajo ausencia de ataques
zdec[t, ] <- zdec[t-1, ] - delta_hat * zdec[t-1, ]
tdec[t, ] <- inv_logit(alpha_hat + beta_hat * zdec[t, ])
}
# Resumimos y ordenamos
pred_z <- rbind(
apply(zinc, 1, mean_ci) |> t(),
apply(zdec, 1, mean_ci) |> t()
) |> as.data.frame()
pred_z$escenario <- rep(c("Ataque", "No ataque"), each = ntpred)
pred_z$Tiempo <- (0:(ntpred-1)) * 5
pred_theta <- rbind(
apply(tinc, 1, mean_ci) |> t(),
apply(tdec, 1, mean_ci) |> t()
) |> as.data.frame()
pred_theta$escenario <- rep(c("Ataque", "No ataque"), each = ntpred)
pred_theta$Tiempo <- (0:(ntpred-1)) * 5
# z
ggplot(pred_z, aes(Tiempo, mean, ymin = lower, ymax = upper,
fill = escenario, color = escenario)) +
geom_ribbon(color = NA, alpha = 0.3) +
geom_line() +
geom_point() +
scale_color_viridis(discrete = T, option = "C", end = 0.5) +
scale_fill_viridis(discrete = T, option = "C", end = 0.5) +
nice_theme() +
theme(legend.title = element_blank()) +
ylab("z") +
xlab("Tiempo (min)") +
ylim(0, 1)
# theta
ggplot(pred_theta, aes(Tiempo, mean, ymin = lower, ymax = upper,
fill = escenario, color = escenario)) +
geom_ribbon(color = NA, alpha = 0.3) +
geom_line() +
geom_point() +
scale_color_viridis(discrete = T, option = "C", end = 0.5) +
scale_fill_viridis(discrete = T, option = "C", end = 0.5) +
nice_theme() +
theme(legend.title = element_blank()) +
ylab(expression(theta)) +
xlab("Tiempo (min)") +
ylim(0, 1)
